{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with some imports and the utility script from the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.4 s, sys: 1.71 s, total: 4.12 s\n",
      "Wall time: 3.08 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "## this script transports l5kit and dependencies\n",
    "os.system('pip uninstall typing -y')\n",
    "os.system('pip install --target=/kaggle/working pymap3d==2.1.0')\n",
    "os.system('pip install --target=/kaggle/working protobuf==3.12.2')\n",
    "os.system('pip install --target=/kaggle/working transforms3d')\n",
    "os.system('pip install --target=/kaggle/working zarr')\n",
    "os.system('pip install --target=/kaggle/working ptable')\n",
    "\n",
    "os.system('pip install --no-dependencies --target=/kaggle/working l5kit')\n",
    "#!pip install --upgrade pip\n",
    "#!pip install pymap3d==2.1.0\n",
    "#!pip install -U l5kit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing some stuff from the l5kit and setting the directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "DIR_INPUT = \"/home/moriarty/Downloads/lyft\"\n",
    "import os\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\n",
    "SINGLE_MODE_SUBMISSION = f\"{DIR_INPUT}/single_mode_sample_submission.csv\"\n",
    "MULTI_MODE_SUBMISSION = f\"{DIR_INPUT}/multi_mode_sample_submission.csv\"\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.evaluation import write_pred_csv\n",
    "from l5kit.rasterization import build_rasterizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config File (dictionary)  from the linked notebook.  This contains parameters for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False  # True just trains for 10 steps instead of the full dataset\n",
    "cfg = {\n",
    "    'format_version': 4,\n",
    "    'model_params': {\n",
    "        'model_architecture': 'resnet50',\n",
    "        'history_num_frames': 20,\n",
    "        'history_step_size': 1,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': 50,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1\n",
    "    },\n",
    "    \n",
    "    'raster_params': {\n",
    "        'raster_size': [224, 224],\n",
    "        'pixel_size': [0.5, 0.5],\n",
    "        'ego_center': [0.25, 0.5],\n",
    "        'map_type': 'py_semantic',\n",
    "        'satellite_map_key': 'aerial_map/aerial_map.png',\n",
    "        'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "        'dataset_meta_key': 'meta.json',\n",
    "        'filter_agents_threshold': 0.5\n",
    "    },\n",
    "    \n",
    "    'train_data_loader': {\n",
    "        'key': 'scenes/train.zarr',\n",
    "        'batch_size': 12,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "    \n",
    "    'train_params': {\n",
    "        'max_num_steps': 10*1 if DEBUG else 3200,\n",
    "        'checkpoint_every_n_steps': 5000,\n",
    "        'train_batch' : 32,\n",
    "        'num_batch' : 10\n",
    "        \n",
    "        # 'eval_every_n_steps': -1\n",
    "    },\n",
    "    \n",
    "    'test_data_loader': {\n",
    "        'key': 'scenes/test.zarr',\n",
    "        'batch_size': 8,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "    \n",
    "    \n",
    "    \n",
    "    'valid_data_loader': {\n",
    "        'key': 'scenes/validate.zarr',\n",
    "        'batch_size': 8,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load In the train dataset.  I notice the pytorch folks can just import this with DataLoader, but I am not familiar with anything similar in keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
      "|   16265    |  4039527   | 320124624  |    38735988   |      112.19     |        248.36        |        79.25         |        24.83         |        10.00        |\n",
      "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moriarty/.local/lib/python3.8/site-packages/l5kit/dataset/agent.py:115: RuntimeWarning: disable_traffic_light_faces not found in config, this will raise an error in the future\n",
      "  return self.get_frame(scene_index, state_index, track_id=track_id)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_cfg = cfg[\"train_data_loader\"]\n",
    "\n",
    "# Rasterizer\n",
    "dm = LocalDataManager(None)\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "\n",
    "# Train dataset/dataloader\n",
    "\n",
    "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
    "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "hist_shape = train_dataset[0]['history_positions'].shape\n",
    "num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "num_in_channels = 3 + num_history_channels\n",
    "num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n",
    "\n",
    "print(train_dataset)\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path1 = dm.require(cfg[\"valid_data_loader\"][\"key\"])\n",
    "valid_zarr = ChunkedDataset(dataset_path1).open()\n",
    "valid_dataset = AgentDataset(cfg, valid_zarr, rasterizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:17<00:00,  5.66it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_itr = iter(valid_dataset)\n",
    "n_valid = 100\n",
    "\n",
    "val_inputs = np.zeros(shape=(n_valid,224,224, num_in_channels) )\n",
    "val_targets = np.zeros(shape=(n_valid,num_targets))\n",
    "for itr in tqdm(range(n_valid)):\n",
    "    data = next(valid_itr)\n",
    "\n",
    "    val_inputs[itr] = data['image'].transpose(1,2,0)    \n",
    "    val_targets[itr] = data['target_positions'].reshape(-1,num_targets)\n",
    "    gc.collect()\n",
    "del valid_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22496709"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2)\n"
     ]
    }
   ],
   "source": [
    "idx = 100\n",
    "plt.scatter(train_dataset[idx]['history_positions'][:,0],train_dataset[idx]['history_positions'][:,1])\n",
    "plt.scatter(train_dataset[idx]['target_positions'][:,0],train_dataset[idx]['target_positions'][:,1],c='r')\n",
    "plt.show()\n",
    "print(train_dataset[0]['target_positions'].shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model architecture.\n",
    "\n",
    "Given what the examples use, lets start with ResNet50.  I tried to initialize with imagenet weights, but I get dimension mismatches, so that is something to look into.\n",
    "\n",
    "The avg pooling layer after ResNet50 will be 2048, so I add in some Dense+ BN + dropout layers, with the last one being 2xnum_targets units.\n",
    "\n",
    "First run will just use sgd optimizer with mse loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.utils.conv_utils import convert_kernel\n",
    "from keras.layers import (Input, Conv2D, Flatten,Dense,AveragePooling2D,Dropout,MaxPooling2D,BatchNormalization)\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.applications.resnet import ResNet101\n",
    "base_in = Input(shape=(224,224,num_in_channels))\n",
    "base_model=Conv2D(32,kernel_size=1,use_bias=False,padding=\"same\")(base_in)\n",
    "base_model=Conv2D(3,kernel_size=3,use_bias=False,padding=\"same\")(base_model)\n",
    "\n",
    "\n",
    "base_model = ResNet101(include_top=False,\n",
    "                       weights='imagenet',\n",
    "                       input_tensor=Input(shape=(224, 224, 3)),\n",
    "                       pooling='max'\n",
    "                       )(base_model)\n",
    "\n",
    "#dense_model = base_model.output\n",
    "dense_model = Dense(1000, activation=\"linear\")(base_model)\n",
    "dense_model = Dropout(.2)(dense_model)\n",
    "dense_model = Dense(500, activation=\"linear\")(dense_model)\n",
    "dense_model = Dropout(.2)(dense_model)\n",
    "dense_model = Dense(num_targets, activation=\"linear\")(dense_model)\n",
    "\n",
    "model = Model(inputs=base_in, outputs=dense_model)\n",
    "opt = optimizers.Adam(lr=0.03)\n",
    "model.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to loop through the train_dataset and use a batch_size variable to train the model in batches.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "MC = ModelCheckpoint('./model.h5', verbose=True,monitor='val_loss',mode='min',\n",
    "        save_weights_only=True,save_best_only=True)\n",
    "\n",
    "stop = EarlyStopping(monitor = 'val_loss', restore_best_weights=True , patience = 5)\n",
    "\n",
    "tr_it = iter(train_dataset)\n",
    "batch_size = cfg['train_params']['train_batch']\n",
    "#progress_bar = tqdm(range(0,cfg[\"train_params\"][\"max_num_steps\"],batch_size))\n",
    "progress_bar = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]))\n",
    "two_hours = 60 * 60 * 2.3\n",
    "losses = []\n",
    "hist = []\n",
    "for itr in progress_bar:#range(0,cfg[\"train_params\"][\"max_num_steps\"],batch_size):\n",
    "    inputs = np.zeros(shape=(batch_size,224,224,num_in_channels))\n",
    "    targets = np.zeros(shape=(batch_size, num_targets))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        try:\n",
    "            data = next(tr_it)\n",
    "        except StopIteration:\n",
    "            tr_it = iter(train_dataset)\n",
    "            data = next(tr_it)\n",
    "            \n",
    "        inputs[i] = data['image'].transpose(1,2,0)\n",
    "        targets[i] = data['target_positions'].reshape(-1,num_targets)\n",
    "   \n",
    "    h = model.fit(inputs, targets,\n",
    "                  batch_size = int(batch_size / 2) ,\n",
    "                  validation_data = (val_inputs, val_targets),\n",
    "                  verbose = 1,\n",
    "                 callbacks = [MC, stop])\n",
    "                  \n",
    "    hist.append(h.history)\n",
    "    gc.collect()\n",
    "    # For training + submission, break if training exceeds 6 hours\n",
    "    if (time.time()-t0) > two_hours:\n",
    "        break\n",
    "    \n",
    "    \n",
    "vl = [hi['val_loss'] for hi in hist]\n",
    "l = [hi['loss'] for hi in hist]\n",
    "plt.plot(np.log(vl), label = 'val_loss')\n",
    "plt.plot(np.log(l), label = 'loss')\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modelv0.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 45)]    0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 224, 224, 32)      1440      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 3)       864       \n",
      "_________________________________________________________________\n",
      "resnet101 (Functional)       (None, 2048)              42658176  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              2049000   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               50100     \n",
      "=================================================================\n",
      "Total params: 45,260,080\n",
      "Trainable params: 45,154,736\n",
      "Non-trainable params: 105,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from numpy import loadtxt\n",
    "from keras.models import load_model\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.utils.conv_utils import convert_kernel\n",
    "from keras.layers import (Input, Conv2D, Flatten,Dense,AveragePooling2D,Dropout,MaxPooling2D,BatchNormalization)\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras.applications.resnet import ResNet101\n",
    "# load model\n",
    "model = load_model('modelv0.h5')\n",
    "# summarize model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f31644041c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Example Prediction:\n",
    "import matplotlib.pyplot as plt\n",
    "tr_it = iter(train_dataset)\n",
    "a1 = next(tr_it)\n",
    "inp = a1['image'].transpose(1,2,0)\n",
    "act = a1['target_positions']\n",
    "pred = model.predict(inp.reshape(-1,224,224,num_in_channels)).reshape(50,2)\n",
    "plt.scatter(act[:,0], act[:,1])\n",
    "plt.scatter(pred[:,0],pred[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this model to predict from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-540cf822d61a>:9: RuntimeWarning: you're running with a custom agents_mask\n",
      "  test_dataset = AgentDataset(cfg, test_zarr, rasterizer, agents_mask=test_mask)\n"
     ]
    }
   ],
   "source": [
    "test_cfg = cfg[\"test_data_loader\"]\n",
    "\n",
    "# Rasterizer\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "\n",
    "# Test dataset/dataloader\n",
    "test_zarr = ChunkedDataset(dm.require(test_cfg[\"key\"])).open()\n",
    "test_mask = np.load(f\"{DIR_INPUT}/scenes/mask.npz\")[\"arr_0\"]\n",
    "test_dataset = AgentDataset(cfg, test_zarr, rasterizer, agents_mask=test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71122/71122 [5:45:33<00:00,  3.43it/s]   \n"
     ]
    }
   ],
   "source": [
    "t_shape = test_dataset[0][\"target_positions\"].shape\n",
    "timestamps = []\n",
    "agent_ids = []\n",
    "coords = []\n",
    "for it in tqdm(test_dataset):\n",
    "    \n",
    "    dat = it['image'].transpose(1,2,0)\n",
    "    coords.append(np.array(model.predict(dat.reshape(1,224,224,num_in_channels)).reshape(t_shape)))\n",
    "    timestamps.append(it[\"timestamp\"])\n",
    "    agent_ids.append(it[\"track_id\"])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l5kit.evaluation import write_pred_csv\n",
    "\n",
    "\n",
    "write_pred_csv('submission.csv',\n",
    "                timestamps = np.array(timestamps),\n",
    "                track_ids = np.array(agent_ids),\n",
    "                coords = np.array(coords) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will play with some parameters later.  I am finally glad to just have something that functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
